import os
import getpass
from typing import TypedDict, List
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")


# Install required libraries
# pip install langchain-groq langchain-community pypdf faiss-cpu sentence-transformers

# Define graph state
class GraphState(TypedDict):
    question: str
    context: List[Document]
    answer: str
    chat_history: list

# 1. Data Ingestion: Load, Chunk, and Embed a PDF
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

def ingest_data(pdf_path: str):
    """Loads a PDF, chunks the text, and creates a FAISS vector store."""
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    
    # Use a free, open-source embedding model from Hugging Face
    print("---LOADING OPEN-SOURCE EMBEDDING MODEL---")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    print("---CREATING VECTOR STORE---")
    vector_store = FAISS.from_documents(documents=splits, embedding=embeddings)
    return vector_store

# 2. Define the RAG Graph Nodes
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END

# Define the nodes for the graph
def retrieve(state: GraphState):
    """Retrieves relevant documents based on the user's question."""
    print("---RETRIEVING DOCUMENTS---")
    question = state["question"]
    retriever = vector_store.as_retriever()
    retrieved_docs = retriever.invoke(question)
    return {"context": retrieved_docs, "question": question}

def generate(state: GraphState):
    """Generates an answer based on the retrieved documents using Groq."""
    print("---GENERATING ANSWER WITH GROQ---")
    question = state["question"]
    context = state["context"]
    chat_history=state["chat_history"]
    
    # Use the Groq API for the LLM
    groq_api_key = os.environ.get("GROQ_API_KEY")
    if not groq_api_key:
        groq_api_key = getpass.getpass("Enter your Groq API key: ")
    
    llm = ChatGroq(
        temperature=0, 
        groq_api_key=groq_api_key, 
        model_name="llama-3.1-8b-instant" # Or another supported Groq model
    )
    
    # Define a prompt template that includes chat history
    prompt_with_history = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant. Answer the question based ONLY on the following context: {context}"),
        ("system", "Conversation history: {chat_history}"), # Add this line
        ("human", "{question}"),
    ])
    
    rag_chain = prompt_with_history | llm
    
    response = rag_chain.invoke({"context": context, "question": question, "chat_history": chat_history})
    return {"answer": response.content, "question": question, "context": context, "chat_history": chat_history}

# 3. Build and Compile the LangGraph
def build_and_run_rag_graph(vector_store_instance, user_question, chat_history):
    """Builds and runs the LangGraph for a user query."""
    global vector_store
    vector_store = vector_store_instance
    
    builder = StateGraph(GraphState)
    builder.add_node("retrieve", retrieve)
    builder.add_node("generate", generate)
    
    builder.add_edge(START, "retrieve")
    builder.add_edge("retrieve", "generate")
    builder.add_edge("generate", END)
    
    graph = builder.compile()

    print(f"User Query: {user_question}")
    result = graph.invoke({"question": user_question, "chat_history": chat_history})
    return result

# The updated main block with a conversational loop
if __name__ == "__main__":
    pdf_path = "discovery.pdf" 
    
    try:
        vector_store_instance = ingest_data(pdf_path)
        
        # Initialize an empty list to store chat history
        chat_history = []
        
        print("Chatbot initiated. Type 'exit' to quit.")
        
        # Start the conversational loop
        while True:
            user_query = input("\nYour question: ")
            
            if user_query.lower() == 'exit':
                print("Goodbye!")
                break
                
            # Pass the chat history to the graph's initial state
            rag_result = build_and_run_rag_graph(
                vector_store_instance,
                user_query,
                chat_history # Pass the history
            )
            
            # Print the final answer
            print("\nFinal Answer:")
            print(rag_result["answer"])
            
            # Update chat history for the next turn
            # You might need to format this to a more readable string or a list of messages
            chat_history.append({"user": user_query, "assistant": rag_result["answer"]})
            

    except FileNotFoundError:
        print(f"Error: The file '{pdf_path}' was not found. Please provide a valid PDF file path.")
    except Exception as e:
        print(f"An error occurred: {e}")